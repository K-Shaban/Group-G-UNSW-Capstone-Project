{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import starting packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "\n",
    "# import datasets from the unzipped CSVs\n",
    "actual_demand_file_path = '../data/totaldemand_nsw.csv'\n",
    "temperature_file_path = '../data/temperature_nsw.csv'\n",
    "actual_demand = pd.read_csv(actual_demand_file_path, parse_dates=['DATETIME'])\n",
    "temperature = pd.read_csv(temperature_file_path, parse_dates=['DATETIME'])\n",
    "forecast_demand_file_path = '../data/forecastdemand_nsw.csv'\n",
    "forecast_demand = pd.read_csv(forecast_demand_file_path)\n",
    "\n",
    "Exports_file_path = '../data/Exports - quarterly 2010-2023.csv'\n",
    "GDP_file_path = '../data/GDP - quarterly 2010-2023.csv'\n",
    "Interest_file_path = '../data/Interest rate - daily 2011-2023.csv'\n",
    "SolarInstall_file_path = '../data/Solar installation number - monthly 2007-2023.csv'\n",
    "SolarOutput_file_path = '../data/Solar outputs - monthly 2015-2023.csv'\n",
    "Unemployment_file_path = '../data/Unemployment rate - monthly 2010-2023.csv'\n",
    "Wholesale_file_path = '../data/Wholesale electricity price - half hourly 2014-2023.csv'\n",
    "cpi_file_path = '../data/inflation-data.xls'\n",
    "comm_file_path = '../data/Commodity prices index - monthly 2010-2023.csv'\n",
    "rainfall_file_path = '../data/Rainfall - daily 2010-2023.csv'\n",
    "weather_file_path = '../data/Weather - daily 2022-2023.csv'\n",
    "public_holiday_path = '../data/nsw_public_holidays.csv'\n",
    "\n",
    "cpi = pd.read_excel(cpi_file_path)\n",
    "Exports = pd.read_csv(Exports_file_path)\n",
    "GDP = pd.read_csv(GDP_file_path)\n",
    "Interest = pd.read_csv(Interest_file_path)\n",
    "SolarInstall = pd.read_csv(SolarInstall_file_path)\n",
    "SolarOutput = pd.read_csv(SolarOutput_file_path)\n",
    "Unemployment = pd.read_csv(Unemployment_file_path)\n",
    "Wholesale = pd.read_csv(Wholesale_file_path)\n",
    "Comm = pd.read_csv(comm_file_path)\n",
    "Rainfall = pd.read_csv(rainfall_file_path)\n",
    "Weather = pd.read_csv(weather_file_path)\n",
    "public_holidays = pd.read_csv(public_holiday_path, parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_value = -6\n",
    "# test_date = '2020-11-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this for 2022 and 2023 data combining\n",
    "# path = '../data/nswdemand2022onwards'\n",
    "# all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# total_demand_2022onwards = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
    "# total_demand_2022onwards.head()\n",
    "\n",
    "\n",
    "# # Quick removal of certain columns and removal of incorrect -9999 values from the temperature data\n",
    "# actual_demand.drop(columns='REGIONID', inplace=True)\n",
    "# temperature.loc[temperature['TEMPERATURE'] <= -5, 'TEMPERATURE'] = np.nan\n",
    "# temperature.drop(columns='LOCATION', inplace=True)\n",
    "\n",
    "# # Combine temperature and demand\n",
    "# df = pd.merge(actual_demand,temperature,left_on=['DATETIME'], right_on=['DATETIME'], how = 'left')\n",
    "\n",
    "# # Interpolate missing temperature values\n",
    "# df['temperature'] = df['TEMPERATURE'].interpolate(method='linear')\n",
    "\n",
    "# # Remove extra temperature column and edit column names\n",
    "# df.drop(columns='TEMPERATURE', inplace= True)\n",
    "# df.rename(columns={'DATETIME': 'datetime', 'TOTALDEMAND':'demand'}, inplace=True)\n",
    "\n",
    "\n",
    "# # Create date and time variables using the datetime column\n",
    "# df['Is weekday'] = np.where(df['datetime'].dt.dayofweek <= 4, 1, 0)\n",
    "# df['Is weekend'] = np.where(df['datetime'].dt.dayofweek > 4, 1, 0)\n",
    "\n",
    "# for i in range(0,24):\n",
    "#     df[f'hour_{i}'] = (df['datetime'].dt.hour == i).astype(int)\n",
    "\n",
    "# for i in range(1,13):\n",
    "#     df[f'month_{i}'] = (df['datetime'].dt.month == i).astype(int)\n",
    "\n",
    "\n",
    "# ## CREATE Lag and summary features for use in models\n",
    "# # create lag of demand and summaries of the demand lag to use as features in the model\n",
    "# df['demand_5_min_lag'] = df['demand'].shift(1)\n",
    "# df['demand_30_min_lag'] = df['demand'].shift(6)\n",
    "# df['demand_1_hr_lag'] = df['demand'].shift(12)\n",
    "# df['demand_24_hr_lag'] = df['demand'].shift(12*24)\n",
    "# df['demand_30_min_mean'] = df['demand'].rolling(window = 6).mean()\n",
    "# df['demand_1_hr_mean'] = df['demand'].rolling(window = 12).mean()\n",
    "# df['demand_24_hr_mean'] = df['demand'].rolling(window = 12*24).mean()\n",
    "# df['demand_30_min_std'] = df['demand'].rolling(window = 6).std()\n",
    "# df['demand_1_hr_std'] = df['demand'].rolling(window = 12).std()\n",
    "# df['demand_24_hr_std'] = df['demand'].rolling(window = 12*24).std()\n",
    "# df['demand_30_min_max'] = df['demand'].rolling(window = 6).max()\n",
    "# df['demand_1_hr_max'] = df['demand'].rolling(window = 12).max()\n",
    "# df['demand_24_hr_max'] = df['demand'].rolling(window = 12*24).max()\n",
    "# df['demand_30_min_min'] = df['demand'].rolling(window = 6).min()\n",
    "# df['demand_1_hr_min'] = df['demand'].rolling(window = 12).min()\n",
    "# df['demand_24_hr_min'] = df['demand'].rolling(window = 12*24).min()\n",
    "# # create lag of temperature and summaries of the temperature lag to use as features in the model\n",
    "# df['temperature_5_min_lag'] = df['temperature'].shift(1)\n",
    "# df['temperature_30_min_lag'] = df['temperature'].shift(6)\n",
    "# df['temperature_1_hr_lag'] = df['temperature'].shift(12)\n",
    "# df['temperature_24_hr_lag'] = df['temperature'].shift(12*24)\n",
    "# df['temperature_30_min_mean'] = df['temperature'].rolling(window = 6).mean()\n",
    "# df['temperature_1_hr_mean'] = df['temperature'].rolling(window = 12).mean()\n",
    "# df['temperature_24_hr_mean'] = df['temperature'].rolling(window = 12*24).mean()\n",
    "# df['temperature_30_min_std'] = df['temperature'].rolling(window = 6).std()\n",
    "# df['temperature_1_hr_std'] = df['temperature'].rolling(window = 12).std()\n",
    "# df['temperature_24_hr_std'] = df['temperature'].rolling(window = 12*24).std()\n",
    "# df['temperature_30_min_max'] = df['temperature'].rolling(window = 6).max()\n",
    "# df['temperature_1_hr_max'] = df['temperature'].rolling(window = 12).max()\n",
    "# df['temperature_24_hr_max'] = df['temperature'].rolling(window = 12*24).max()\n",
    "# df['temperature_30_min_min'] = df['temperature'].rolling(window = 6).min()\n",
    "# df['temperature_1_hr_min'] = df['temperature'].rolling(window = 12).min()\n",
    "# df['temperature_24_hr_min'] = df['temperature'].rolling(window = 12*24).min()\n",
    "\n",
    "# Add public holidays to the dataframe\n",
    "# public_holiday_path = '../data/nsw_public_holidays.csv'\n",
    "# public_holidays = pd.read_csv(public_holiday_path, parse_dates=['Date'])\n",
    "# df['date'] = pd.to_datetime(df['datetime'].dt.date)\n",
    "# df = pd.merge(df,public_holidays,left_on=['date'], right_on=['Date'], how = 'left')\n",
    "# df['regular day'] = np.where(df['Holiday Name'].isnull(), 1, 0)\n",
    "# df['public holiday'] = np.where(df['Holiday Name'].isnull(), 0, 1)\n",
    "# df.drop(columns=['date', 'Date', 'Holiday Name'], inplace = True)\n",
    "\n",
    "# Write to final_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Holiday Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>New Year's Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2014-01-27</td>\n",
       "      <td>Australia Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-18</td>\n",
       "      <td>Good Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2014-04-19</td>\n",
       "      <td>Easter Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2014-04-20</td>\n",
       "      <td>Easter Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>Anzac Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>King's Birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>2023-10-02</td>\n",
       "      <td>Labour Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>2023-12-25</td>\n",
       "      <td>Christmas Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>Boxing Day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date     Holiday Name\n",
       "0   2014-01-01   New Year's Day\n",
       "1   2014-01-27    Australia Day\n",
       "2   2014-04-18      Good Friday\n",
       "3   2014-04-19  Easter Saturday\n",
       "4   2014-04-20    Easter Sunday\n",
       "..         ...              ...\n",
       "113 2023-04-25        Anzac Day\n",
       "114 2023-06-12  King's Birthday\n",
       "115 2023-10-02       Labour Day\n",
       "116 2023-12-25   Christmas Day \n",
       "117 2023-12-26       Boxing Day\n",
       "\n",
       "[118 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public_holidays = public_holidays.rename(columns=column_rename_mapping).dropna()\n",
    "# public_holidays['DATETIME'] = pd.to_datetime(public_holidays['DATETIME'].dt.date)\n",
    "# public_holidays = public_holidays.set_index('DATETIME')\n",
    "# df = pd.merge(df,public_holidays, how = 'left')\n",
    "# df['regular day'] = np.where(df['Holiday Name'].isnull(), 1, 0)\n",
    "# df['public holiday'] = np.where(df['Holiday Name'].isnull(), 0, 1)\n",
    "# df.drop(columns=['Date', 'Holiday Name'], inplace = True)\n",
    "\n",
    "# column_rename_mapping = {\n",
    "#     'Date': 'DATETIME',\n",
    "# }\n",
    "# public_holidays = public_holidays.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "# public_holidays.index = pd.to_datetime(public_holidays.index)\n",
    "# date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "# min_date, max_date = df.index.min(), df.index.max()\n",
    "# public_holidays = public_holidays[(public_holidays.index >= min_date) & (public_holidays.index <= max_date)]\n",
    "# public_holidays = public_holidays.reindex(date_range).fillna(method='ffill')\n",
    "# # df = pd.concat([df, cpi], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'min_date' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-609cb258854f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmin_date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'min_date' is not defined"
     ]
    }
   ],
   "source": [
    "min_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature.loc[temperature['TEMPERATURE'] <= -5, 'TEMPERATURE'] = np.nan\n",
    "df = pd.DataFrame({'Date Time': actual_demand['DATETIME'], 'Total Demand': actual_demand['TOTALDEMAND']})\n",
    "df = pd.merge(df,temperature,left_on=['Date Time'], right_on=['DATETIME'], how = 'left')\n",
    "df=df.drop(columns=['DATETIME', 'LOCATION'])\n",
    "df['TEMPERATURE'] = df['TEMPERATURE'].interpolate(method='linear')\n",
    "df['Date Time'] = pd.to_datetime(df['Date Time'])\n",
    "forecast_demand['DATETIME'] = pd.to_datetime(forecast_demand['DATETIME'])\n",
    "actual_demand['DATETIME'] = pd.to_datetime(actual_demand['DATETIME'])\n",
    "forecast_demand['DATETIME'] = pd.to_datetime(forecast_demand['DATETIME'])\n",
    "actual_demand['DATETIME'] = pd.to_datetime(actual_demand['DATETIME'])\n",
    "fore = forecast_demand.set_index('DATETIME').resample('1H').mean()[['FORECASTDEMAND']]\n",
    "actu = actual_demand.set_index('DATETIME').resample('1H').mean()[['TOTALDEMAND']]\n",
    "merged_df = fore.join(actu).dropna()\n",
    "merged_df['DIFF'] = merged_df.eval('TOTALDEMAND - FORECASTDEMAND')\n",
    "merged_df['Demand_Previous'] = merged_df['TOTALDEMAND'].shift(periods=(-24))\n",
    "merged_df['Demand_Lagged'] = merged_df['TOTALDEMAND'].shift(periods=(lag_value))\n",
    "merged_df['Demand_Rolling'] = merged_df['Demand_Lagged'].rolling(window=720).mean()\n",
    "\n",
    "temperature['DATETIME'] = pd.to_datetime(temperature['DATETIME'])\n",
    "temp = temperature.set_index('DATETIME').resample('1H').mean()\n",
    "temp['TEMPERATURE_lagged'] = temp['TEMPERATURE'].shift(periods=lag_value)\n",
    "data = merged_df.merge(temp, left_index=True, right_index=True, how='inner')\n",
    "data = data.drop(['LOCATION','TEMPERATURE', 'FORECASTDEMAND', 'DIFF'], axis=1)\n",
    "df = data.dropna()\n",
    "\n",
    "\n",
    "column_rename_mapping = {\n",
    "    'G1 CONSUMER PRICE INFLATION': 'DATETIME',\n",
    "    'Unnamed: 1': 'cpi'\n",
    "}\n",
    "cpi = cpi.rename(columns=column_rename_mapping)[10:].dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "cpi.index = pd.to_datetime(cpi.index, format='%b-%Y')\n",
    "cpi = cpi[(cpi.index >= min_date) & (cpi.index <= max_date)]\n",
    "cpi = cpi.reindex(date_range).fillna(method='ffill')\n",
    "cpi['CPI_lagged'] = cpi['cpi'].shift(periods=lag_value)\n",
    "cpi = cpi[['CPI_lagged']]\n",
    "df = pd.concat([df, cpi], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    'Date': 'DATETIME',\n",
    "    'Exports of goods and services': 'Exports'\n",
    "}\n",
    "Exports = Exports.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "Exports.index = pd.to_datetime(Exports.index, format='%b-%Y')\n",
    "Exports = Exports[(Exports.index >= min_date) & (Exports.index <= max_date)]\n",
    "Exports = Exports.reindex(date_range).fillna(method='ffill')\n",
    "Exports['Exports_lagged'] = Exports['Exports'].shift(periods=lag_value)\n",
    "Exports = Exports[['Exports_lagged']]\n",
    "df = pd.concat([df, Exports], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    'Date': 'DATETIME',\n",
    "    'Real GDP': 'GDP'\n",
    "}\n",
    "GDP = GDP.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "GDP.index = pd.to_datetime(GDP.index, format='%b-%Y')\n",
    "GDP = GDP[(GDP.index >= min_date) & (GDP.index <= max_date)]\n",
    "GDP = GDP.reindex(date_range).fillna(method='ffill')\n",
    "GDP['GDP_lagged'] = GDP['GDP'].shift(periods=lag_value)\n",
    "GDP = GDP[['GDP_lagged']]\n",
    "df = pd.concat([df, GDP], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    'Date': 'DATETIME',\n",
    "    'Cash Rate Target': 'Interest'\n",
    "}\n",
    "Interest = Interest.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "Interest.index = pd.to_datetime(Interest.index)\n",
    "Interest = Interest[(Interest.index >= min_date) & (Interest.index <= max_date)]\n",
    "Interest = Interest.reindex(date_range).fillna(method='ffill')\n",
    "Interest['Interest_lagged'] = Interest['Interest'].shift(periods=lag_value)\n",
    "Interest = Interest[['Interest_lagged']]\n",
    "df = pd.concat([df, Interest], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    \"'Month'\": 'DATETIME',\n",
    "    'Cumulative Residential': 'Residential',\n",
    "    'Cumulative Commercial': 'Commercial'\n",
    "}\n",
    "SolarInstall = SolarInstall.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "SolarInstall.index = pd.to_datetime(SolarInstall.index)\n",
    "SolarInstall = SolarInstall[(SolarInstall.index >= min_date) & (SolarInstall.index <= max_date)]\n",
    "SolarInstall = SolarInstall.reindex(date_range).fillna(method='ffill')\n",
    "SolarInstall['Residential_lagged'] = SolarInstall['Residential'].shift(periods=lag_value)\n",
    "SolarInstall['Commercial_lagged'] = SolarInstall['Commercial'].shift(periods=lag_value)\n",
    "SolarInstall = SolarInstall[['Residential_lagged','Commercial_lagged']]\n",
    "df = pd.concat([df, SolarInstall], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    \"Date\": 'DATETIME',\n",
    "    'Output NSW': 'SolarOutput'\n",
    "}\n",
    "SolarOutput = SolarOutput.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "SolarOutput.index = pd.to_datetime(SolarOutput.index)\n",
    "SolarOutput = SolarOutput[(SolarOutput.index >= min_date) & (SolarOutput.index <= max_date)]\n",
    "SolarOutput = SolarOutput.reindex(date_range).fillna(method='ffill')\n",
    "SolarOutput['SolarOutput_lagged'] = SolarOutput['SolarOutput'].shift(periods=lag_value)\n",
    "SolarOutput = SolarOutput[['SolarOutput_lagged']]\n",
    "df = pd.concat([df, SolarOutput], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    \"Date\": 'DATETIME',\n",
    "    'Unemployment rate': 'Unemployment'\n",
    "}\n",
    "Unemployment = Unemployment.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "Unemployment.index = pd.to_datetime(Unemployment.index, format='%b-%Y')\n",
    "Unemployment = Unemployment[(Unemployment.index >= min_date) & (Unemployment.index <= max_date)]\n",
    "Unemployment = Unemployment.reindex(date_range).fillna(method='ffill')\n",
    "Unemployment['Unemployment_lagged'] = Unemployment['Unemployment'].shift(periods=lag_value)\n",
    "Unemployment = Unemployment[['Unemployment_lagged']]\n",
    "df = pd.concat([df, Unemployment], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    \"SETTLEMENTDATE\": 'DATETIME',\n",
    "    'RRP': 'Price'\n",
    "}\n",
    "Wholesale = Wholesale.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "Wholesale.index = pd.to_datetime(Wholesale.index, format='%Y/%m/%d %H:%M:%S')\n",
    "Wholesale = Wholesale[(Wholesale.index >= min_date) & (Wholesale.index <= max_date)]\n",
    "Wholesale = Wholesale.reindex(date_range).fillna(method='ffill')\n",
    "Wholesale['Price_lagged'] = Wholesale['Price'].shift(periods=lag_value)\n",
    "Wholesale = Wholesale[['Price_lagged']]\n",
    "df = pd.concat([df, Wholesale], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    \"Date\": 'DATETIME',\n",
    "    'Commodity prices ': 'CommPrice'\n",
    "}\n",
    "Comm = Comm.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "Comm.index = pd.to_datetime(Comm.index, format='%b-%Y')\n",
    "Comm = Comm[(Comm.index >= min_date) & (Comm.index <= max_date)]\n",
    "Comm = Comm.reindex(date_range).fillna(method='ffill')\n",
    "Comm['CommPrice_lagged'] = Comm['CommPrice'].shift(periods=lag_value)\n",
    "Comm = Comm[['CommPrice_lagged']]\n",
    "df = pd.concat([df, Comm], axis=1).dropna()\n",
    "df['HourOfDay'] = df.index.hour\n",
    "day_mapping = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "df['DayOfWeek'] = df.index.dayofweek.map(day_mapping)\n",
    "df['DayOfYear'] = df.index.dayofyear\n",
    "df['IsWeekend'] = np.where((df['DayOfWeek'] == 5) | (df['DayOfWeek'] == 6), 1, 0)\n",
    "# df = pd.get_dummies(df, columns=['HourOfDay'], prefix='Hour')\n",
    "# df = pd.get_dummies(df, columns=['DayOfWeek'], prefix='Day')\n",
    "# df = pd.get_dummies(df, columns=['DayOfYear'], prefix='Year')\n",
    "# df = pd.get_dummies(df, columns=['IsWeekend'], prefix='Weekend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/final_data.csv',index=False, mode = 'w')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
