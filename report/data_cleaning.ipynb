{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import starting packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "\n",
    "# import datasets from the unzipped CSVs\n",
    "actual_demand_file_path = '../data/totaldemand_nsw.csv'\n",
    "temperature_file_path = '../data/temperature_nsw.csv'\n",
    "actual_demand = pd.read_csv(actual_demand_file_path, parse_dates=['DATETIME'])\n",
    "temperature = pd.read_csv(temperature_file_path, parse_dates=['DATETIME'])\n",
    "forecast_demand_file_path = '../data/forecastdemand_nsw.csv'\n",
    "forecast_demand = pd.read_csv(forecast_demand_file_path)\n",
    "\n",
    "Exports_file_path = '../data/Exports - quarterly 2010-2023.csv'\n",
    "GDP_file_path = '../data/GDP - quarterly 2010-2023.csv'\n",
    "Interest_file_path = '../data/Interest rate - daily 2011-2023.csv'\n",
    "SolarInstall_file_path = '../data/Solar installation number - monthly 2007-2023.csv'\n",
    "SolarOutput_file_path = '../data/Solar outputs - monthly 2015-2023.csv'\n",
    "Unemployment_file_path = '../data/Unemployment rate - monthly 2010-2023.csv'\n",
    "Wholesale_file_path = '../data/Wholesale electricity price - half hourly 2014-2023.csv'\n",
    "cpi_file_path = '../data/inflation-data.xls'\n",
    "comm_file_path = '../data/Commodity prices index - monthly 2010-2023.csv'\n",
    "rainfall_file_path = '../data/Rainfall - daily 2010-2023.csv'\n",
    "weather_file_path = '../data/Weather - daily 2022-2023.csv'\n",
    "public_holiday_path = '../data/nsw_public_holidays.csv'\n",
    "\n",
    "cpi = pd.read_excel(cpi_file_path)\n",
    "Exports = pd.read_csv(Exports_file_path)\n",
    "GDP = pd.read_csv(GDP_file_path)\n",
    "Interest = pd.read_csv(Interest_file_path)\n",
    "SolarInstall = pd.read_csv(SolarInstall_file_path)\n",
    "SolarOutput = pd.read_csv(SolarOutput_file_path)\n",
    "Unemployment = pd.read_csv(Unemployment_file_path)\n",
    "Wholesale = pd.read_csv(Wholesale_file_path)\n",
    "Comm = pd.read_csv(comm_file_path)\n",
    "Rainfall = pd.read_csv(rainfall_file_path)\n",
    "Weather = pd.read_csv(weather_file_path)\n",
    "public_holidays = pd.read_csv(public_holiday_path, parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_value = -6\n",
    "# test_date = '2020-11-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this for 2022 and 2023 data combining\n",
    "# path = '../data/nswdemand2022onwards'\n",
    "# all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# total_demand_2022onwards = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
    "# total_demand_2022onwards.head()\n",
    "\n",
    "\n",
    "# # Quick removal of certain columns and removal of incorrect -9999 values from the temperature data\n",
    "# actual_demand.drop(columns='REGIONID', inplace=True)\n",
    "# temperature.loc[temperature['TEMPERATURE'] <= -5, 'TEMPERATURE'] = np.nan\n",
    "# temperature.drop(columns='LOCATION', inplace=True)\n",
    "\n",
    "# # Combine temperature and demand\n",
    "# df = pd.merge(actual_demand,temperature,left_on=['DATETIME'], right_on=['DATETIME'], how = 'left')\n",
    "\n",
    "# # Interpolate missing temperature values\n",
    "# df['temperature'] = df['TEMPERATURE'].interpolate(method='linear')\n",
    "\n",
    "# # Remove extra temperature column and edit column names\n",
    "# df.drop(columns='TEMPERATURE', inplace= True)\n",
    "# df.rename(columns={'DATETIME': 'datetime', 'TOTALDEMAND':'demand'}, inplace=True)\n",
    "\n",
    "\n",
    "# # Create date and time variables using the datetime column\n",
    "# df['Is weekday'] = np.where(df['datetime'].dt.dayofweek <= 4, 1, 0)\n",
    "# df['Is weekend'] = np.where(df['datetime'].dt.dayofweek > 4, 1, 0)\n",
    "\n",
    "# for i in range(0,24):\n",
    "#     df[f'hour_{i}'] = (df['datetime'].dt.hour == i).astype(int)\n",
    "\n",
    "# for i in range(1,13):\n",
    "#     df[f'month_{i}'] = (df['datetime'].dt.month == i).astype(int)\n",
    "\n",
    "\n",
    "# ## CREATE Lag and summary features for use in models\n",
    "# # create lag of demand and summaries of the demand lag to use as features in the model\n",
    "# df['demand_5_min_lag'] = df['demand'].shift(1)\n",
    "# df['demand_30_min_lag'] = df['demand'].shift(6)\n",
    "# df['demand_1_hr_lag'] = df['demand'].shift(12)\n",
    "# df['demand_24_hr_lag'] = df['demand'].shift(12*24)\n",
    "# df['demand_30_min_mean'] = df['demand'].rolling(window = 6).mean()\n",
    "# df['demand_1_hr_mean'] = df['demand'].rolling(window = 12).mean()\n",
    "# df['demand_24_hr_mean'] = df['demand'].rolling(window = 12*24).mean()\n",
    "# df['demand_30_min_std'] = df['demand'].rolling(window = 6).std()\n",
    "# df['demand_1_hr_std'] = df['demand'].rolling(window = 12).std()\n",
    "# df['demand_24_hr_std'] = df['demand'].rolling(window = 12*24).std()\n",
    "# df['demand_30_min_max'] = df['demand'].rolling(window = 6).max()\n",
    "# df['demand_1_hr_max'] = df['demand'].rolling(window = 12).max()\n",
    "# df['demand_24_hr_max'] = df['demand'].rolling(window = 12*24).max()\n",
    "# df['demand_30_min_min'] = df['demand'].rolling(window = 6).min()\n",
    "# df['demand_1_hr_min'] = df['demand'].rolling(window = 12).min()\n",
    "# df['demand_24_hr_min'] = df['demand'].rolling(window = 12*24).min()\n",
    "# # create lag of temperature and summaries of the temperature lag to use as features in the model\n",
    "# df['temperature_5_min_lag'] = df['temperature'].shift(1)\n",
    "# df['temperature_30_min_lag'] = df['temperature'].shift(6)\n",
    "# df['temperature_1_hr_lag'] = df['temperature'].shift(12)\n",
    "# df['temperature_24_hr_lag'] = df['temperature'].shift(12*24)\n",
    "# df['temperature_30_min_mean'] = df['temperature'].rolling(window = 6).mean()\n",
    "# df['temperature_1_hr_mean'] = df['temperature'].rolling(window = 12).mean()\n",
    "# df['temperature_24_hr_mean'] = df['temperature'].rolling(window = 12*24).mean()\n",
    "# df['temperature_30_min_std'] = df['temperature'].rolling(window = 6).std()\n",
    "# df['temperature_1_hr_std'] = df['temperature'].rolling(window = 12).std()\n",
    "# df['temperature_24_hr_std'] = df['temperature'].rolling(window = 12*24).std()\n",
    "# df['temperature_30_min_max'] = df['temperature'].rolling(window = 6).max()\n",
    "# df['temperature_1_hr_max'] = df['temperature'].rolling(window = 12).max()\n",
    "# df['temperature_24_hr_max'] = df['temperature'].rolling(window = 12*24).max()\n",
    "# df['temperature_30_min_min'] = df['temperature'].rolling(window = 6).min()\n",
    "# df['temperature_1_hr_min'] = df['temperature'].rolling(window = 12).min()\n",
    "# df['temperature_24_hr_min'] = df['temperature'].rolling(window = 12*24).min()\n",
    "\n",
    "# Add public holidays to the dataframe\n",
    "# public_holiday_path = '../data/nsw_public_holidays.csv'\n",
    "# public_holidays = pd.read_csv(public_holiday_path, parse_dates=['Date'])\n",
    "# df['date'] = pd.to_datetime(df['datetime'].dt.date)\n",
    "# df = pd.merge(df,public_holidays,left_on=['date'], right_on=['Date'], how = 'left')\n",
    "# df['regular day'] = np.where(df['Holiday Name'].isnull(), 1, 0)\n",
    "# df['public holiday'] = np.where(df['Holiday Name'].isnull(), 0, 1)\n",
    "# df.drop(columns=['date', 'Date', 'Holiday Name'], inplace = True)\n",
    "\n",
    "# Write to final_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public_holidays = public_holidays.rename(columns=column_rename_mapping).dropna()\n",
    "# public_holidays['DATETIME'] = pd.to_datetime(public_holidays['DATETIME'].dt.date)\n",
    "# public_holidays = public_holidays.set_index('DATETIME')\n",
    "# df = pd.merge(df,public_holidays, how = 'left')\n",
    "# df['regular day'] = np.where(df['Holiday Name'].isnull(), 1, 0)\n",
    "# df['public holiday'] = np.where(df['Holiday Name'].isnull(), 0, 1)\n",
    "# df.drop(columns=['Date', 'Holiday Name'], inplace = True)\n",
    "\n",
    "# column_rename_mapping = {\n",
    "#     'Date': 'DATETIME',\n",
    "# }\n",
    "# public_holidays = public_holidays.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "# public_holidays.index = pd.to_datetime(public_holidays.index)\n",
    "# date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "# min_date, max_date = df.index.min(), df.index.max()\n",
    "# public_holidays = public_holidays[(public_holidays.index >= min_date) & (public_holidays.index <= max_date)]\n",
    "# public_holidays = public_holidays.reindex(date_range).fillna(method='ffill')\n",
    "# # df = pd.concat([df, cpi], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature.loc[temperature['TEMPERATURE'] <= -5, 'TEMPERATURE'] = np.nan\n",
    "df = pd.DataFrame({'Date Time': actual_demand['DATETIME'], 'Total Demand': actual_demand['TOTALDEMAND']})\n",
    "df = pd.merge(df,temperature,left_on=['Date Time'], right_on=['DATETIME'], how = 'left')\n",
    "df=df.drop(columns=['DATETIME', 'LOCATION'])\n",
    "df['TEMPERATURE'] = df['TEMPERATURE'].interpolate(method='linear')\n",
    "df['Date Time'] = pd.to_datetime(df['Date Time'])\n",
    "forecast_demand['DATETIME'] = pd.to_datetime(forecast_demand['DATETIME'])\n",
    "actual_demand['DATETIME'] = pd.to_datetime(actual_demand['DATETIME'])\n",
    "forecast_demand['DATETIME'] = pd.to_datetime(forecast_demand['DATETIME'])\n",
    "actual_demand['DATETIME'] = pd.to_datetime(actual_demand['DATETIME'])\n",
    "fore = forecast_demand.set_index('DATETIME').resample('1H').mean()[['FORECASTDEMAND']]\n",
    "actu = actual_demand.set_index('DATETIME').resample('1H').mean()[['TOTALDEMAND']]\n",
    "merged_df = fore.join(actu).dropna()\n",
    "merged_df['DIFF'] = merged_df.eval('TOTALDEMAND - FORECASTDEMAND')\n",
    "merged_df['Demand_Previous'] = merged_df['TOTALDEMAND'].shift(periods=(-24))\n",
    "merged_df['Demand_Lagged'] = merged_df['TOTALDEMAND'].shift(periods=(lag_value))\n",
    "merged_df['Demand_Rolling'] = merged_df['Demand_Lagged'].rolling(window=720).mean()\n",
    "\n",
    "temperature['DATETIME'] = pd.to_datetime(temperature['DATETIME'])\n",
    "temp = temperature.set_index('DATETIME').resample('1H').mean()\n",
    "temp['TEMPERATURE_lagged'] = temp['TEMPERATURE'].shift(periods=lag_value)\n",
    "data = merged_df.merge(temp, left_index=True, right_index=True, how='inner')\n",
    "data = data.drop(['LOCATION','TEMPERATURE', 'FORECASTDEMAND', 'DIFF'], axis=1)\n",
    "df = data.dropna()\n",
    "\n",
    "\n",
    "column_rename_mapping = {\n",
    "    'G1 CONSUMER PRICE INFLATION': 'DATETIME',\n",
    "    'Unnamed: 1': 'cpi'\n",
    "}\n",
    "cpi = cpi.rename(columns=column_rename_mapping)[10:].dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "cpi.index = pd.to_datetime(cpi.index, format='%b-%Y')\n",
    "cpi = cpi[(cpi.index >= min_date) & (cpi.index <= max_date)]\n",
    "cpi = cpi.reindex(date_range).fillna(method='ffill')\n",
    "cpi['CPI_lagged'] = cpi['cpi'].shift(periods=lag_value)\n",
    "cpi = cpi[['CPI_lagged']]\n",
    "df = pd.concat([df, cpi], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    'Date': 'DATETIME',\n",
    "    'Exports of goods and services': 'Exports'\n",
    "}\n",
    "Exports = Exports.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "Exports.index = pd.to_datetime(Exports.index, format='%b-%Y')\n",
    "Exports = Exports[(Exports.index >= min_date) & (Exports.index <= max_date)]\n",
    "Exports = Exports.reindex(date_range).fillna(method='ffill')\n",
    "Exports['Exports_lagged'] = Exports['Exports'].shift(periods=lag_value)\n",
    "Exports = Exports[['Exports_lagged']]\n",
    "df = pd.concat([df, Exports], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    'Date': 'DATETIME',\n",
    "    'Real GDP': 'GDP'\n",
    "}\n",
    "GDP = GDP.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "GDP.index = pd.to_datetime(GDP.index, format='%b-%Y')\n",
    "GDP = GDP[(GDP.index >= min_date) & (GDP.index <= max_date)]\n",
    "GDP = GDP.reindex(date_range).fillna(method='ffill')\n",
    "GDP['GDP_lagged'] = GDP['GDP'].shift(periods=lag_value)\n",
    "GDP = GDP[['GDP_lagged']]\n",
    "df = pd.concat([df, GDP], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    'Date': 'DATETIME',\n",
    "    'Cash Rate Target': 'Interest'\n",
    "}\n",
    "Interest = Interest.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "Interest.index = pd.to_datetime(Interest.index)\n",
    "Interest = Interest[(Interest.index >= min_date) & (Interest.index <= max_date)]\n",
    "Interest = Interest.reindex(date_range).fillna(method='ffill')\n",
    "Interest['Interest_lagged'] = Interest['Interest'].shift(periods=lag_value)\n",
    "Interest = Interest[['Interest_lagged']]\n",
    "df = pd.concat([df, Interest], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    \"'Month'\": 'DATETIME',\n",
    "    'Cumulative Residential': 'Residential',\n",
    "    'Cumulative Commercial': 'Commercial'\n",
    "}\n",
    "SolarInstall = SolarInstall.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "SolarInstall.index = pd.to_datetime(SolarInstall.index)\n",
    "SolarInstall = SolarInstall[(SolarInstall.index >= min_date) & (SolarInstall.index <= max_date)]\n",
    "SolarInstall = SolarInstall.reindex(date_range).fillna(method='ffill')\n",
    "SolarInstall['Residential_lagged'] = SolarInstall['Residential'].shift(periods=lag_value)\n",
    "SolarInstall['Commercial_lagged'] = SolarInstall['Commercial'].shift(periods=lag_value)\n",
    "SolarInstall = SolarInstall[['Residential_lagged','Commercial_lagged']]\n",
    "df = pd.concat([df, SolarInstall], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    \"Date\": 'DATETIME',\n",
    "    'Output NSW': 'SolarOutput'\n",
    "}\n",
    "SolarOutput = SolarOutput.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "SolarOutput.index = pd.to_datetime(SolarOutput.index)\n",
    "SolarOutput = SolarOutput[(SolarOutput.index >= min_date) & (SolarOutput.index <= max_date)]\n",
    "SolarOutput = SolarOutput.reindex(date_range).fillna(method='ffill')\n",
    "SolarOutput['SolarOutput_lagged'] = SolarOutput['SolarOutput'].shift(periods=lag_value)\n",
    "SolarOutput = SolarOutput[['SolarOutput_lagged']]\n",
    "df = pd.concat([df, SolarOutput], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    \"Date\": 'DATETIME',\n",
    "    'Unemployment rate': 'Unemployment'\n",
    "}\n",
    "Unemployment = Unemployment.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "Unemployment.index = pd.to_datetime(Unemployment.index, format='%b-%Y')\n",
    "Unemployment = Unemployment[(Unemployment.index >= min_date) & (Unemployment.index <= max_date)]\n",
    "Unemployment = Unemployment.reindex(date_range).fillna(method='ffill')\n",
    "Unemployment['Unemployment_lagged'] = Unemployment['Unemployment'].shift(periods=lag_value)\n",
    "Unemployment = Unemployment[['Unemployment_lagged']]\n",
    "df = pd.concat([df, Unemployment], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    \"SETTLEMENTDATE\": 'DATETIME',\n",
    "    'RRP': 'Price'\n",
    "}\n",
    "Wholesale = Wholesale.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "Wholesale.index = pd.to_datetime(Wholesale.index, format='%Y/%m/%d %H:%M:%S')\n",
    "Wholesale = Wholesale[(Wholesale.index >= min_date) & (Wholesale.index <= max_date)]\n",
    "Wholesale = Wholesale.reindex(date_range).fillna(method='ffill')\n",
    "Wholesale['Price_lagged'] = Wholesale['Price'].shift(periods=lag_value)\n",
    "Wholesale = Wholesale[['Price_lagged']]\n",
    "df = pd.concat([df, Wholesale], axis=1).dropna()\n",
    "\n",
    "column_rename_mapping = {\n",
    "    \"Date\": 'DATETIME',\n",
    "    'Commodity prices ': 'CommPrice'\n",
    "}\n",
    "Comm = Comm.rename(columns=column_rename_mapping).dropna().set_index('DATETIME')\n",
    "min_date, max_date = data.index.min(), data.index.max()\n",
    "date_range = pd.date_range(start=min_date, end=max_date,freq='1H')\n",
    "Comm.index = pd.to_datetime(Comm.index, format='%b-%Y')\n",
    "Comm = Comm[(Comm.index >= min_date) & (Comm.index <= max_date)]\n",
    "Comm = Comm.reindex(date_range).fillna(method='ffill')\n",
    "Comm['CommPrice_lagged'] = Comm['CommPrice'].shift(periods=lag_value)\n",
    "Comm = Comm[['CommPrice_lagged']]\n",
    "df = pd.concat([df, Comm], axis=1).dropna()\n",
    "df['HourOfDay'] = df.index.hour\n",
    "day_mapping = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "df['DayOfWeek'] = df.index.dayofweek.map(day_mapping)\n",
    "df['DayOfYear'] = df.index.dayofyear\n",
    "df['IsWeekend'] = np.where((df['DayOfWeek'] == 5) | (df['DayOfWeek'] == 6), 1, 0)\n",
    "# df = pd.get_dummies(df, columns=['HourOfDay'], prefix='Hour')\n",
    "# df = pd.get_dummies(df, columns=['DayOfWeek'], prefix='Day')\n",
    "# df = pd.get_dummies(df, columns=['DayOfYear'], prefix='Year')\n",
    "# df = pd.get_dummies(df, columns=['IsWeekend'], prefix='Weekend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOTALDEMAND</th>\n",
       "      <th>Demand_Previous</th>\n",
       "      <th>Demand_Lagged</th>\n",
       "      <th>Demand_Rolling</th>\n",
       "      <th>TEMPERATURE_lagged</th>\n",
       "      <th>CPI_lagged</th>\n",
       "      <th>Exports_lagged</th>\n",
       "      <th>GDP_lagged</th>\n",
       "      <th>Interest_lagged</th>\n",
       "      <th>Residential_lagged</th>\n",
       "      <th>Commercial_lagged</th>\n",
       "      <th>SolarOutput_lagged</th>\n",
       "      <th>Unemployment_lagged</th>\n",
       "      <th>Price_lagged</th>\n",
       "      <th>CommPrice_lagged</th>\n",
       "      <th>HourOfDay</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DayOfYear</th>\n",
       "      <th>IsWeekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2015-03-31 18:00:00</td>\n",
       "      <td>8467.307500</td>\n",
       "      <td>8738.001667</td>\n",
       "      <td>6833.315833</td>\n",
       "      <td>7767.094856</td>\n",
       "      <td>14.666667</td>\n",
       "      <td>106.8</td>\n",
       "      <td>1.081540e+11</td>\n",
       "      <td>4.632940e+11</td>\n",
       "      <td>2.25</td>\n",
       "      <td>765282.0</td>\n",
       "      <td>291300.0</td>\n",
       "      <td>11308.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>33.59</td>\n",
       "      <td>45.6</td>\n",
       "      <td>18</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-03-31 19:00:00</td>\n",
       "      <td>8160.216667</td>\n",
       "      <td>8448.161667</td>\n",
       "      <td>6331.099167</td>\n",
       "      <td>7767.564594</td>\n",
       "      <td>15.825000</td>\n",
       "      <td>106.8</td>\n",
       "      <td>1.081540e+11</td>\n",
       "      <td>4.632940e+11</td>\n",
       "      <td>2.25</td>\n",
       "      <td>765282.0</td>\n",
       "      <td>291300.0</td>\n",
       "      <td>11308.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>27.02</td>\n",
       "      <td>45.6</td>\n",
       "      <td>19</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-03-31 20:00:00</td>\n",
       "      <td>7678.155000</td>\n",
       "      <td>7952.970000</td>\n",
       "      <td>5947.957500</td>\n",
       "      <td>7767.780329</td>\n",
       "      <td>16.283333</td>\n",
       "      <td>106.8</td>\n",
       "      <td>1.081540e+11</td>\n",
       "      <td>4.632940e+11</td>\n",
       "      <td>2.25</td>\n",
       "      <td>765282.0</td>\n",
       "      <td>291300.0</td>\n",
       "      <td>11308.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>27.70</td>\n",
       "      <td>45.6</td>\n",
       "      <td>20</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-03-31 21:00:00</td>\n",
       "      <td>7462.178333</td>\n",
       "      <td>7671.011667</td>\n",
       "      <td>5931.215000</td>\n",
       "      <td>7767.914020</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>106.8</td>\n",
       "      <td>1.081540e+11</td>\n",
       "      <td>4.632940e+11</td>\n",
       "      <td>2.25</td>\n",
       "      <td>765282.0</td>\n",
       "      <td>291300.0</td>\n",
       "      <td>11308.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>20.96</td>\n",
       "      <td>45.6</td>\n",
       "      <td>21</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-03-31 22:00:00</td>\n",
       "      <td>7299.590833</td>\n",
       "      <td>7463.340000</td>\n",
       "      <td>6303.179167</td>\n",
       "      <td>7768.012985</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>106.8</td>\n",
       "      <td>1.081540e+11</td>\n",
       "      <td>4.632940e+11</td>\n",
       "      <td>2.25</td>\n",
       "      <td>765282.0</td>\n",
       "      <td>291300.0</td>\n",
       "      <td>11308.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>24.63</td>\n",
       "      <td>45.6</td>\n",
       "      <td>22</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TOTALDEMAND  Demand_Previous  Demand_Lagged  \\\n",
       "2015-03-31 18:00:00  8467.307500      8738.001667    6833.315833   \n",
       "2015-03-31 19:00:00  8160.216667      8448.161667    6331.099167   \n",
       "2015-03-31 20:00:00  7678.155000      7952.970000    5947.957500   \n",
       "2015-03-31 21:00:00  7462.178333      7671.011667    5931.215000   \n",
       "2015-03-31 22:00:00  7299.590833      7463.340000    6303.179167   \n",
       "\n",
       "                     Demand_Rolling  TEMPERATURE_lagged  CPI_lagged  \\\n",
       "2015-03-31 18:00:00     7767.094856           14.666667       106.8   \n",
       "2015-03-31 19:00:00     7767.564594           15.825000       106.8   \n",
       "2015-03-31 20:00:00     7767.780329           16.283333       106.8   \n",
       "2015-03-31 21:00:00     7767.914020           15.700000       106.8   \n",
       "2015-03-31 22:00:00     7768.012985           14.900000       106.8   \n",
       "\n",
       "                     Exports_lagged    GDP_lagged  Interest_lagged  \\\n",
       "2015-03-31 18:00:00    1.081540e+11  4.632940e+11             2.25   \n",
       "2015-03-31 19:00:00    1.081540e+11  4.632940e+11             2.25   \n",
       "2015-03-31 20:00:00    1.081540e+11  4.632940e+11             2.25   \n",
       "2015-03-31 21:00:00    1.081540e+11  4.632940e+11             2.25   \n",
       "2015-03-31 22:00:00    1.081540e+11  4.632940e+11             2.25   \n",
       "\n",
       "                     Residential_lagged  Commercial_lagged  \\\n",
       "2015-03-31 18:00:00            765282.0           291300.0   \n",
       "2015-03-31 19:00:00            765282.0           291300.0   \n",
       "2015-03-31 20:00:00            765282.0           291300.0   \n",
       "2015-03-31 21:00:00            765282.0           291300.0   \n",
       "2015-03-31 22:00:00            765282.0           291300.0   \n",
       "\n",
       "                     SolarOutput_lagged  Unemployment_lagged  Price_lagged  \\\n",
       "2015-03-31 18:00:00             11308.0                  6.1         33.59   \n",
       "2015-03-31 19:00:00             11308.0                  6.1         27.02   \n",
       "2015-03-31 20:00:00             11308.0                  6.1         27.70   \n",
       "2015-03-31 21:00:00             11308.0                  6.1         20.96   \n",
       "2015-03-31 22:00:00             11308.0                  6.1         24.63   \n",
       "\n",
       "                     CommPrice_lagged  HourOfDay DayOfWeek  DayOfYear  \\\n",
       "2015-03-31 18:00:00              45.6         18   Tuesday         90   \n",
       "2015-03-31 19:00:00              45.6         19   Tuesday         90   \n",
       "2015-03-31 20:00:00              45.6         20   Tuesday         90   \n",
       "2015-03-31 21:00:00              45.6         21   Tuesday         90   \n",
       "2015-03-31 22:00:00              45.6         22   Tuesday         90   \n",
       "\n",
       "                     IsWeekend  \n",
       "2015-03-31 18:00:00          0  \n",
       "2015-03-31 19:00:00          0  \n",
       "2015-03-31 20:00:00          0  \n",
       "2015-03-31 21:00:00          0  \n",
       "2015-03-31 22:00:00          0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('../data/final_data.csv',index=False, mode = 'w')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
